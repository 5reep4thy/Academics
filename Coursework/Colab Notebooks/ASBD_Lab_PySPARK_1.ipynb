{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"ASBD_Lab_PySPARK_1.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"3yh_qH4RusRe"},"source":["## Find Pyspark"]},{"cell_type":"code","metadata":{"id":"YNYkiWlcusRp"},"source":["import findspark\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkCrdXFyusRq"},"source":["### Start a Spark Session"]},{"cell_type":"code","metadata":{"id":"ERknpaGlusRr"},"source":["from pyspark.context import SparkContext\n","sc = SparkContext('local')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LPetpUvIusRr"},"source":["#### TO stop a running spark session"]},{"cell_type":"code","metadata":{"id":"KZxdJUrXusRr"},"source":["#Execute if the following error occurs\"\n","'''\n","Cannot run multiple SparkContexts at once;existing SparkContext(app=pyspark-shell, master=local)created by __init__ at <ipython-input-19-5796b8bfe42c>:3 \n","'''\n","sc.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfjZwsw0usRs","outputId":"885ca0fe-4847-460d-90dc-a471367416ea"},"source":["text_file = sc.textFile(\"data/wordcount.txt\")\n","counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b)\n","#counts.saveAsTextFile(\"hdfs://...\")\n","output = counts.collect()\n","for (word, count) in output:\n","        print(\"%s: %i\" % (word, count))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unless: 1\n","required: 1\n","by: 1\n","applicable: 1\n","law: 1\n","or: 2\n","agreed: 1\n","to: 1\n","in: 1\n","writing,: 1\n","software: 1\n","distributed: 2\n","under: 2\n","the: 4\n","License: 2\n","is: 1\n","on: 1\n","an: 1\n","\"AS: 1\n","IS\": 1\n","BASIS,: 1\n","WITHOUT: 1\n","WARRANTIES: 1\n","OR: 1\n","CONDITIONS: 1\n","OF: 1\n","ANY: 1\n","KIND,: 1\n","either: 1\n","express: 1\n","implied.: 1\n","See: 1\n","for: 1\n","specific: 1\n","language: 1\n","governing: 1\n","permissions: 1\n","and: 1\n","limitations: 1\n","License.: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XpD2CYC1usRw","outputId":"d13236be-de05-469e-f16c-99f26f1da86d"},"source":["import numpy as np\n","\n","from pyspark.mllib.stat import Statistics\n","\n","mat = sc.parallelize(\n","    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])]\n",")  # an RDD of Vectors\n","\n","# Compute column summary statistics.\n","summary = Statistics.colStats(mat)\n","print(summary.mean())  # a dense vector containing the mean value for each column\n","print(summary.variance())  # column-wise variance\n","print(summary.numNonzeros())  # number of nonzeros in each column"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[  2.  20. 200.]\n","[1.e+00 1.e+02 1.e+04]\n","[3. 3. 3.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LMuViCMOusRx","outputId":"2825dcc7-9310-4579-85d5-c1c252aa0deb"},"source":["from pyspark.mllib.stat import Statistics\n","\n","seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n","# seriesY must have the same number of partitions and cardinality as seriesX\n","seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n","\n","# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n","# If a method is not specified, Pearson's method will be used by default.\n","print(\"Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))\n","\n","data = sc.parallelize(\n","    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]\n",")  # an RDD of Vectors\n","\n","# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n","# If a method is not specified, Pearson's method will be used by default.\n","print(Statistics.corr(data, method=\"pearson\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Correlation is: 0.8500286768773004\n","[[1.         0.97888347 0.99038957]\n"," [0.97888347 1.         0.99774832]\n"," [0.99038957 0.99774832 1.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a8sh7JV5usRx"},"source":["# Will start a spark session and then run programs\n","from pyspark.mllib.fpm import FPGrowth\n","# $example off$\n","from pyspark import SparkContext\n","\n","if __name__ == \"__main__\":\n","    sc = SparkContext(appName=\"FPGrowth\")\n","\n","    # $example on$\n","    data = sc.textFile(\"data/mllib/fimi_sample.txt\")\n","    transactions = data.map(lambda line: line.strip().split(' '))\n","    model = FPGrowth.train(transactions, minSupport=0.003, numPartitions=10)\n","    result = model.freqItemsets().collect()\n","    for fi in result:\n","        print(fi)\n","        \n","sc.stop()\n","    # $example off$"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNOSmV8kusRy"},"source":["### FP growth-MLlib"]},{"cell_type":"code","metadata":{"id":"XGoWVcSTusRy","outputId":"630bd368-2782-48bb-f44f-d8cda43e6636"},"source":["from pyspark.ml.fpm import FPGrowth\n","# $example off$\n","from pyspark.sql import SparkSession\n","\n","if __name__ == \"__main__\":\n","    spark = SparkSession\\\n","        .builder\\\n","        .appName(\"FPGrowthExample\")\\\n","        .getOrCreate()\n","\n","    # $example on$\n","    df = spark.createDataFrame([\n","        (0, [1, 2, 5]),\n","        (1, [1, 2, 3, 5]),\n","        (2, [1, 2])\n","    ], [\"id\", \"items\"])\n","\n","    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n","    model = fpGrowth.fit(df)\n","\n","    # Display frequent itemsets.\n","    model.freqItemsets.show()\n","\n","    # Display generated association rules.\n","    model.associationRules.show()\n","\n","    # transform examines the input items against all the association rules and summarize the\n","    # consequents as prediction\n","    model.transform(df).show()\n","    # $example off$\n","\n","    spark.stop()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+----+\n","|    items|freq|\n","+---------+----+\n","|      [1]|   3|\n","|      [2]|   3|\n","|   [2, 1]|   3|\n","|      [5]|   2|\n","|   [5, 2]|   2|\n","|[5, 2, 1]|   2|\n","|   [5, 1]|   2|\n","+---------+----+\n","\n","+----------+----------+------------------+----+\n","|antecedent|consequent|        confidence|lift|\n","+----------+----------+------------------+----+\n","|    [5, 2]|       [1]|               1.0| 1.0|\n","|       [2]|       [1]|               1.0| 1.0|\n","|       [2]|       [5]|0.6666666666666666| 1.0|\n","|    [2, 1]|       [5]|0.6666666666666666| 1.0|\n","|       [5]|       [2]|               1.0| 1.0|\n","|       [5]|       [1]|               1.0| 1.0|\n","|    [5, 1]|       [2]|               1.0| 1.0|\n","|       [1]|       [2]|               1.0| 1.0|\n","|       [1]|       [5]|0.6666666666666666| 1.0|\n","+----------+----------+------------------+----+\n","\n","+---+------------+----------+\n","| id|       items|prediction|\n","+---+------------+----------+\n","|  0|   [1, 2, 5]|        []|\n","|  1|[1, 2, 3, 5]|        []|\n","|  2|      [1, 2]|       [5]|\n","+---+------------+----------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oGsXQ__SusRz"},"source":["### Decision Tree Classification: Gini Impurity"]},{"cell_type":"code","metadata":{"id":"x6EOCPFHusRz","outputId":"fe1fbd84-ada8-48ad-aaa3-98545dfcb28f"},"source":["from pyspark import SparkContext\n","# $example on$\n","from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n","from pyspark.mllib.util import MLUtils\n","# $example off$\n","\n","if __name__ == \"__main__\":\n","\n","    sc = SparkContext(appName=\"PythonDecisionTreeClassificationExample\")\n","\n","    # $example on$\n","    # Load and parse the data file into an RDD of LabeledPoint.\n","    data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n","    # Split the data into training and test sets (30% held out for testing)\n","    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n","\n","    # Train a DecisionTree model.\n","    #  Empty categoricalFeaturesInfo indicates all features are continuous.\n","    model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n","                                         impurity='gini', maxDepth=5, maxBins=32)\n","\n","    # Evaluate model on test instances and compute test error\n","    predictions = model.predict(testData.map(lambda x: x.features))\n","    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n","    testErr = labelsAndPredictions.filter(\n","        lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n","    print('Test Error = ' + str(testErr))\n","    print('Learned classification tree model:')\n","    print(model.toDebugString())\n","\n","    # Save and load model\n","    model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n","    sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n","    # $example off$\n","sc.stop()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Error = 0.037037037037037035\n","Learned classification tree model:\n","DecisionTreeModel classifier of depth 2 with 5 nodes\n","  If (feature 434 <= 70.5)\n","   If (feature 99 <= 35.0)\n","    Predict: 0.0\n","   Else (feature 99 > 35.0)\n","    Predict: 1.0\n","  Else (feature 434 > 70.5)\n","   Predict: 1.0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1DTGGvzIusR0"},"source":["### Naive Bayes Classification"]},{"cell_type":"code","metadata":{"id":"8YusT9puusR0"},"source":["from pyspark.context import SparkContext\n","sc = SparkContext('local')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2cKJhXcusR1","outputId":"a6d83ba6-c9f3-42ba-fa3e-5e57bee4b496"},"source":["from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n","from pyspark.mllib.util import MLUtils\n","\n","\n","# Load and parse the data file.\n","data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n","\n","# Split data approximately into training (60%) and test (40%)\n","training, test = data.randomSplit([0.6, 0.4])\n","\n","# Train a naive Bayes model.\n","model = NaiveBayes.train(training, 1.0)\n","\n","# Make prediction and test accuracy.\n","predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n","accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n","print('model accuracy {}'.format(accuracy))\n","sc.stop()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model accuracy 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g9WEIPQwusR1","outputId":"99fc2d4e-0149-4dca-cb16-e24248136e3e"},"source":["import shutil\n","\n","from pyspark import SparkContext\n","# $example on$\n","from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n","from pyspark.mllib.util import MLUtils\n","\n","\n","# $example off$\n","\n","if __name__ == \"__main__\":\n","\n","    sc = SparkContext(appName=\"PythonNaiveBayesExample\")\n","\n","    # $example on$\n","    # Load and parse the data file.\n","    data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n","\n","    # Split data approximately into training (60%) and test (40%)\n","    training, test = data.randomSplit([0.6, 0.4])\n","\n","    # Train a naive Bayes model.\n","    model = NaiveBayes.train(training, 1.0)\n","\n","    # Make prediction and test accuracy.\n","    predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n","    accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n","    print('model accuracy {}'.format(accuracy))\n","\n","    # Save and load model\n","    output_dir = 'target/tmp/myNaiveBayesModel'\n","    shutil.rmtree(output_dir, ignore_errors=True)\n","    model.save(sc, output_dir)\n","    sameModel = NaiveBayesModel.load(sc, output_dir)\n","    predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))\n","    accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n","    print('sameModel accuracy {}'.format(accuracy))\n","    sc.stop()\n","\n","    # $example off$"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model accuracy 0.975\n","sameModel accuracy 0.975\n"],"name":"stdout"}]}]}